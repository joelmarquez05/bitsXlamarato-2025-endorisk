# GuÃ­a del Proyecto NEST - Para el Equipo

---

## ğŸš€ Quick Start (Lo Primero)

### App Streamlit (Docker)
```bash
docker compose up --build -d
```
â†’ Abre **http://localhost:8501**

### Notebooks (venv local)
```bash
# Primera vez:
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Cada vez que abras el proyecto:
.venv\Scripts\activate
```

### API Key (.env)
Crea archivo `.env` en la raÃ­z:
```
GEMINI_API_KEY=tu_api_key_aqui
```

---

## 1. Â¿QuÃ© estamos construyendo?

**NEST** = Herramienta para predecir la probabilidad de recaÃ­da en pacientes con cÃ¡ncer de endometrio NSMP.

### Producto Final
Una app web (Streamlit) donde:
1. El doctor hace login
2. Introduce datos del paciente
3. Ve la predicciÃ³n + explicaciÃ³n de por quÃ© el modelo predijo eso

---

## 2. Â¿Por quÃ© dividimos el trabajo en notebooks?

### El problema de "todo en una notebook"
Cuando trabajamos todo en una sola notebook:
- âŒ El archivo se hace enorme e inmanejable
- âŒ Si alguien cambia algo, puede romper todo
- âŒ No podemos reutilizar el modelo entrenado en la app
- âŒ Cada vez hay que re-entrenar todo desde cero

### La soluciÃ³n: Dividir y Guardar
```
Notebook 01: Explorar datos â†’ No guarda nada
Notebook 02: Limpiar datos  â†’ GUARDA archivos CSV
Notebook 03: Entrenar modelo â†’ GUARDA modelo .joblib
Notebook 04: Evaluar modelo â†’ CARGA el modelo guardado
App Streamlit              â†’ CARGA el modelo y hace predicciones
```

**Ventaja:** Entrenas el modelo UNA VEZ, lo guardas, y la app lo usa sin re-entrenar.

---

## 3. Â¿QuÃ© es Joblib y por quÃ© lo usamos?

**Joblib** es una librerÃ­a que permite guardar objetos de Python (como modelos de ML) en archivos.

### Â¿Por quÃ© no podemos simplemente "pasarle" el modelo a la app?
Porque las notebooks y la app son procesos separados. No comparten memoria.

### AnalogÃ­a
Imagina que el modelo es un documento de Word:
- Sin guardar: Solo existe mientras Word estÃ¡ abierto
- Guardado (.docx): Puedes cerrarlo y abrirlo luego, o enviarlo a otro PC

Con modelos de ML es igual:
- Sin guardar: Solo existe mientras la notebook estÃ¡ ejecutÃ¡ndose
- Guardado (.joblib): Puedes cerrar la notebook, y la app puede cargarlo

---

## 4. Tutorial: CÃ³mo Guardar y Cargar

### InstalaciÃ³n
```python
# Ya incluido en requirements.txt, pero por si acaso:
pip install joblib
```

### Guardar un modelo (en notebook 03)
```python
import joblib
from sklearn.ensemble import RandomForestClassifier

# Entrenas tu modelo
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Lo guardas en un archivo
joblib.dump(model, '../models/model.joblib')
print("Modelo guardado!")
```

### Cargar un modelo (en la app o notebook 04)
```python
import joblib

# Cargas el modelo desde el archivo
model = joblib.load('../models/model.joblib')

# Ya puedes usarlo para predecir
prediction = model.predict(X_new)
probabilidad = model.predict_proba(X_new)[0][1]
```

---

## 5. Â¿QuÃ© mÃ¡s se puede guardar con Joblib?

| Objeto | Â¿Guardarlo? | Archivo sugerido |
|--------|-------------|------------------|
| Modelo entrenado | âœ… SÃ­ | `models/model.joblib` |
| Scaler (StandardScaler, etc.) | âœ… SÃ­ | `models/scaler.joblib` |
| Encoder (LabelEncoder, etc.) | âœ… SÃ­ | `models/encoder.joblib` |
| DataFrame procesado | âŒ No (usar CSV) | `data/processed/datos.csv` |

### Â¿Por quÃ© guardar el Scaler?
Si normalizas los datos antes de entrenar, necesitas la MISMA normalizaciÃ³n para predecir.

```python
# En notebook 02 (preprocesamiento)
from sklearn.preprocessing import StandardScaler
import joblib

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Ajusta y transforma
joblib.dump(scaler, '../models/scaler.joblib')  # Guarda

# En la app (predicciÃ³n)
import joblib

scaler = joblib.load('models/scaler.joblib')  # Carga
X_new_scaled = scaler.transform(X_new)  # Solo transforma (NO fit!)
prediction = model.predict(X_new_scaled)
```

---

## 6. Estructura del Proyecto

```
bitsXlamarato-2025-pau_overfitting/
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ raw/                    # Datos originales
â”‚   â””â”€â”€ processed/              # Datos limpios (CSV)
â”‚
â”œâ”€â”€ ğŸ“ notebooks/               # Ejecutar EN ORDEN: 01 â†’ 02 â†’ 03 â†’ 04
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb   # Guarda â†’ data/processed/
â”‚   â”œâ”€â”€ 03_model_training.ipynb       # Guarda â†’ models/
â”‚   â””â”€â”€ 04_model_evaluation.ipynb
â”‚
â”œâ”€â”€ ğŸ“ models/                  # AquÃ­ se guardan los .joblib
â”‚   â”œâ”€â”€ model.joblib
â”‚   â””â”€â”€ scaler.joblib
â”‚
â”œâ”€â”€ ğŸ“ app/                     # App Streamlit
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ agent.MD                    # Instrucciones para la IA
â””â”€â”€ guia.MD                     # Este archivo (para el equipo)
```

---

## 7. Flujo de Trabajo Paso a Paso

### DÃ­a 1: PreparaciÃ³n
```
1. Clonar el repo
2. docker-compose up --build
3. Abrir notebooks en JupyterLab o VS Code
```

### Flujo de Notebooks
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  01_exploratory_analysis.ipynb                              â”‚
â”‚  - Ver distribuciones, valores nulos, correlaciones         â”‚
â”‚  - NO guarda nada                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  02_data_preprocessing.ipynb                                â”‚
â”‚  - Limpiar datos, crear features, split train/test          â”‚
â”‚  - GUARDA:                                                  â”‚
â”‚    â€¢ data/processed/features_train.csv                      â”‚
â”‚    â€¢ data/processed/features_test.csv                       â”‚
â”‚    â€¢ models/scaler.joblib                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  03_model_training.ipynb                                    â”‚
â”‚  - CARGA: data/processed/features_train.csv                 â”‚
â”‚  - Entrenar modelo(s), comparar mÃ©tricas                    â”‚
â”‚  - GUARDA: models/model.joblib                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  04_model_evaluation.ipynb                                  â”‚
â”‚  - CARGA: models/model.joblib, features_test.csv            â”‚
â”‚  - Evaluar en test, matrices de confusiÃ³n, mÃ©tricas         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  app/main.py (Streamlit)                                    â”‚
â”‚  - CARGA: models/model.joblib, models/scaler.joblib         â”‚
â”‚  - Recibe input del doctor                                  â”‚
â”‚  - Muestra predicciÃ³n + explicaciÃ³n (Gemini API)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. Setup del Entorno

Tenemos **DOS entornos** separados:

| Entorno | Uso | Â¿CuÃ¡ndo? |
|---------|-----|----------|
| **Docker** | App Streamlit | Siempre para ver/probar la app |
| **venv local** | Notebooks Jupyter | Para desarrollo de ML |

---

### 8.1 Docker (App Streamlit)

**Ãšnico comando que necesitas:** 
```bash
docker compose up --build -d
```

> ğŸ’¡ El flag `-d` hace que corra en segundo plano (no bloquea tu terminal)

La app estarÃ¡ disponible en: **http://localhost:8501**

#### Otros comandos Ãºtiles:
```bash
# Ver logs del contenedor
docker compose logs -f

# Parar el contenedor
docker compose down

# Reconstruir si cambiaste algo
docker compose up --build -d
```

---

### 8.2 Entorno Virtual Local (Notebooks)

Para trabajar con las notebooks necesitas un entorno Python local.

#### Primera vez (crear el venv):
```bash
# Crear el entorno virtual
python -m venv .venv

# Activar el entorno (Windows PowerShell)
.venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

#### Cada vez que abras el proyecto:
```bash
# Solo necesitas activar el venv
.venv\Scripts\activate
```

> ğŸ“ SabrÃ¡s que estÃ¡ activo cuando veas `(.venv)` al inicio de tu terminal

#### En VS Code:
1. Abre una notebook
2. Click en "Select Kernel" (arriba a la derecha)
3. Selecciona `.venv` o "Python 3.x (.venv)"

---

### 8.3 Archivo `.env` (API Keys)

Crea un archivo llamado `.env` en la raÃ­z del proyecto:

```
GEMINI_API_KEY=tu_api_key_real_aqui
```

> ğŸ“ Obtener API key: https://makersuite.google.com/app/apikey

âš ï¸ **IMPORTANTE:** Este archivo estÃ¡ en `.gitignore` y NO se sube a GitHub.

---

## 9. Resumen RÃ¡pido

| AcciÃ³n | Comando/CÃ³digo |
|--------|----------------|
| Guardar modelo | `joblib.dump(model, 'models/model.joblib')` |
| Cargar modelo | `model = joblib.load('models/model.joblib')` |
| Guardar CSV | `df.to_csv('data/processed/datos.csv', index=False)` |
| Cargar CSV | `df = pd.read_csv('data/processed/datos.csv')` |
| Levantar Docker | `docker-compose up --build` |
| Ejecutar app | `streamlit run app/main.py` |

---

## 10. Preguntas Frecuentes

### Â¿Por quÃ© los paths tienen `../`?
Porque las notebooks estÃ¡n en `notebooks/`, y necesitan "subir" un nivel para acceder a `models/` o `data/`.

```
notebooks/03_training.ipynb  â†’  quiere guardar en  â†’  models/model.joblib
                â†“ sube un nivel
       ../models/model.joblib  âœ“
```

### Â¿QuÃ© pasa si modifico el preprocesamiento?
Tienes que re-ejecutar notebooks 02, 03, 04 en orden. El modelo antiguo ya no serÃ¡ vÃ¡lido.

### Â¿El modelo se guarda automÃ¡ticamente?
NO. Tienes que ejecutar la celda con `joblib.dump()` explÃ­citamente.
