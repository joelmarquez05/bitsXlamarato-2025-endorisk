{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Model Training\n",
                "\n",
                "**Purpose**: Train models, compare, save best one.\n",
                "\n",
                "**Inputs**: `data/processed/features_v1_*.csv`\n",
                "\n",
                "**Outputs**: `models/model_v1.joblib`\n",
                "\n",
                "‚ö†Ô∏è **MEDICAL**: We prioritize RECALL - missing a recurrence is FATAL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ö†Ô∏è imbalanced-learn no instal¬∑lat. Executa: pip install imbalanced-learn\n"
                    ]
                }
            ],
            "source": [
                "\"\"\"\n",
                "03_model_training.ipynb\n",
                "Entrenament de models per predicci√≥ de recidiva en c√†ncer d'endometri.\n",
                "Focus: Maximitzar SENSIBILITAT (Recall) - Un Fals Negatiu √©s fatal!\n",
                "\"\"\"\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, roc_auc_score,\n",
                "    recall_score, precision_score, f1_score, make_scorer\n",
                ")\n",
                "\n",
                "# XGBoost\n",
                "try:\n",
                "    import xgboost as xgb\n",
                "    XGBOOST_AVAILABLE = True\n",
                "except ImportError:\n",
                "    print(\"‚ö†Ô∏è XGBoost no instal¬∑lat. Executa: pip install xgboost\")\n",
                "    XGBOOST_AVAILABLE = False\n",
                "\n",
                "# SMOTE per balanceig\n",
                "try:\n",
                "    from imblearn.over_sampling import SMOTE\n",
                "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "    SMOTE_AVAILABLE = True\n",
                "except ImportError:\n",
                "    print(\"‚ö†Ô∏è imbalanced-learn no instal¬∑lat. Executa: pip install imbalanced-learn\")\n",
                "    SMOTE_AVAILABLE = False\n",
                "\n",
                "# Constants\n",
                "RANDOM_STATE = 42\n",
                "PROCESSED_DATA_PATH = '../data/processed/dataset_procesado.csv'\n",
                "MODELS_PATH = '../models/'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyError",
                    "evalue": "\"['recidiva'] not found in axis\"",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m df = pd.read_csv(PROCESSED_DATA_PATH)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Separar X i y\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecidiva\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mrecidiva\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoj\\OneDrive\\Escritorio\\Hackathon\\bitsXlamarato-2025-pau_overfitting\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:5603\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5456\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5457\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5464\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5465\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5467\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5601\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5602\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5609\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5610\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5611\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoj\\OneDrive\\Escritorio\\Hackathon\\bitsXlamarato-2025-pau_overfitting\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4810\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4810\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4813\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoj\\OneDrive\\Escritorio\\Hackathon\\bitsXlamarato-2025-pau_overfitting\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4852\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4850\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4851\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4852\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4853\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4855\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoj\\OneDrive\\Escritorio\\Hackathon\\bitsXlamarato-2025-pau_overfitting\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
                        "\u001b[31mKeyError\u001b[39m: \"['recidiva'] not found in axis\""
                    ]
                }
            ],
            "source": [
                "# C√†rrega del dataset processat\n",
                "df = pd.read_csv(PROCESSED_DATA_PATH)\n",
                "\n",
                "# Separar X i y\n",
                "X = df.drop(columns=['recidiva'])\n",
                "y = df['recidiva']\n",
                "\n",
                "print(f\"üìä Dataset shape: {X.shape}\")\n",
                "print(f\"\\nüéØ Distribuci√≥ del target:\")\n",
                "print(y.value_counts())\n",
                "print(f\"\\n‚ö†Ô∏è Ratio desbalanceig: {y.value_counts()[0] / y.value_counts()[1]:.2f}:1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split estratificat per mantenir proporci√≥ del target\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=0.2, \n",
                "    random_state=RANDOM_STATE,\n",
                "    stratify=y  # Cr√≠tic per datasets desbalancejats\n",
                ")\n",
                "\n",
                "print(f\"Train: {X_train.shape[0]} mostres\")\n",
                "print(f\"Test: {X_test.shape[0]} mostres\")\n",
                "print(f\"\\nDistribuci√≥ train: {y_train.value_counts().to_dict()}\")\n",
                "print(f\"Distribuci√≥ test: {y_test.value_counts().to_dict()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Escalar features (important per Logistic Regression)\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Guardar scaler\n",
                "import os\n",
                "os.makedirs(MODELS_PATH, exist_ok=True)\n",
                "joblib.dump(scaler, f'{MODELS_PATH}scaler_v1.joblib')\n",
                "print(f\"‚úÖ Scaler guardat a: {MODELS_PATH}scaler_v1.joblib\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(model, X_test, y_test, model_name):\n",
                "    \"\"\"\n",
                "    Avaluaci√≥ amb focus m√®dic: prioritzem SENSIBILITAT (Recall).\n",
                "    Un Fals Negatiu (no detectar recidiva) √©s pitjor que un Fals Positiu.\n",
                "    \"\"\"\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"üìã RESULTATS: {model_name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # M√®triques\n",
                "    recall = recall_score(y_test, y_pred)\n",
                "    precision = precision_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    \n",
                "    print(f\"\\nüéØ SENSIBILITAT (Recall): {recall:.3f}  ‚Üê PRIORITAT M√ÄXIMA\")\n",
                "    print(f\"üìä Precisi√≥: {precision:.3f}\")\n",
                "    print(f\"‚öñÔ∏è F1-Score: {f1:.3f}\")\n",
                "    \n",
                "    if y_proba is not None:\n",
                "        auc = roc_auc_score(y_test, y_proba)\n",
                "        print(f\"üìà ROC-AUC: {auc:.3f}\")\n",
                "    \n",
                "    # Matriu de confusi√≥\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    print(f\"\\nüìä Matriu de Confusi√≥:\")\n",
                "    print(f\"   TN={cm[0,0]}  FP={cm[0,1]}\")\n",
                "    print(f\"   FN={cm[1,0]}  TP={cm[1,1]}\")\n",
                "    \n",
                "    # Interpretaci√≥ m√®dica\n",
                "    fn = cm[1,0]\n",
                "    if fn > 0:\n",
                "        print(f\"\\n‚ö†Ô∏è ALERTA: {fn} Falsos Negatius (pacients amb recidiva no detectada)\")\n",
                "    else:\n",
                "        print(f\"\\n‚úÖ Cap Fals Negatiu!\")\n",
                "    \n",
                "    print(f\"\\n{classification_report(y_test, y_pred, target_names=['No Recidiva', 'Recidiva'])}\")\n",
                "    \n",
                "    return {'recall': recall, 'precision': precision, 'f1': f1, 'model': model}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Logistic Regression amb class_weight per compensar desbalanceig\n",
                "log_reg = LogisticRegression(\n",
                "    class_weight='balanced',  # Ajusta pesos autom√†ticament\n",
                "    max_iter=1000,\n",
                "    random_state=RANDOM_STATE,\n",
                "    solver='lbfgs'\n",
                ")\n",
                "\n",
                "log_reg.fit(X_train_scaled, y_train)\n",
                "results_lr = evaluate_model(log_reg, X_test_scaled, y_test, \"Logistic Regression\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest amb class_weight\n",
                "rf_model = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    class_weight='balanced',  # Important!\n",
                "    max_depth=5,              # Evitar overfitting (poques mostres)\n",
                "    min_samples_leaf=5,       # Regularitzaci√≥\n",
                "    random_state=RANDOM_STATE,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "rf_model.fit(X_train, y_train)  # RF no necessita escalat\n",
                "results_rf = evaluate_model(rf_model, X_test, y_test, \"Random Forest\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if XGBOOST_AVAILABLE:\n",
                "    # Calcular scale_pos_weight per desbalanceig\n",
                "    scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
                "    \n",
                "    xgb_model = xgb.XGBClassifier(\n",
                "        scale_pos_weight=scale_pos_weight,  # Compensa desbalanceig\n",
                "        max_depth=3,                         # Evitar overfitting\n",
                "        learning_rate=0.1,\n",
                "        n_estimators=100,\n",
                "        min_child_weight=5,                  # Regularitzaci√≥\n",
                "        subsample=0.8,\n",
                "        colsample_bytree=0.8,\n",
                "        random_state=RANDOM_STATE,\n",
                "        eval_metric='logloss',\n",
                "        use_label_encoder=False\n",
                "    )\n",
                "    \n",
                "    xgb_model.fit(X_train, y_train)\n",
                "    results_xgb = evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è XGBoost no disponible\")\n",
                "    results_xgb = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comparar models per Recall (prioritat m√®dica)\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üèÜ COMPARACI√ì FINAL (Ordenat per RECALL)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "results = [\n",
                "    (\"Logistic Regression\", results_lr),\n",
                "    (\"Random Forest\", results_rf),\n",
                "]\n",
                "if results_xgb:\n",
                "    results.append((\"XGBoost\", results_xgb))\n",
                "\n",
                "# Ordenar per recall (descendent)\n",
                "results_sorted = sorted(results, key=lambda x: x[1]['recall'], reverse=True)\n",
                "\n",
                "for i, (name, res) in enumerate(results_sorted):\n",
                "    emoji = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\"\n",
                "    print(f\"{emoji} {name}: Recall={res['recall']:.3f}, F1={res['f1']:.3f}\")\n",
                "\n",
                "# Seleccionar millor model\n",
                "best_name, best_result = results_sorted[0]\n",
                "best_model = best_result['model']\n",
                "print(f\"\\n‚úÖ Millor model seleccionat: {best_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Guardar el millor model\n",
                "model_filename = f'{MODELS_PATH}model_v1.joblib'\n",
                "joblib.dump(best_model, model_filename)\n",
                "print(f\"‚úÖ Model guardat a: {model_filename}\")\n",
                "\n",
                "# Guardar feature names per l'app\n",
                "feature_names = list(X.columns)\n",
                "joblib.dump(feature_names, f'{MODELS_PATH}feature_names_v1.joblib')\n",
                "print(f\"‚úÖ Feature names guardats\")\n",
                "\n",
                "print(f\"\\nüìã Features utilitzades:\")\n",
                "for i, feat in enumerate(feature_names, 1):\n",
                "    print(f\"   {i}. {feat}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Feature importance del Random Forest\n",
                "if hasattr(best_model, 'feature_importances_'):\n",
                "    importances = best_model.feature_importances_\n",
                "elif hasattr(best_model, 'coef_'):\n",
                "    importances = np.abs(best_model.coef_[0])\n",
                "else:\n",
                "    importances = None\n",
                "\n",
                "if importances is not None:\n",
                "    feat_imp = pd.DataFrame({\n",
                "        'feature': feature_names,\n",
                "        'importance': importances\n",
                "    }).sort_values('importance', ascending=True)\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    plt.barh(feat_imp['feature'], feat_imp['importance'], color='steelblue')\n",
                "    plt.xlabel('Import√†ncia')\n",
                "    plt.title('Import√†ncia de les Features')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Step\n",
                "‚Üí Go to `04_model_evaluation.ipynb`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
